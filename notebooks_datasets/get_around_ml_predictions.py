# -*- coding: utf-8 -*-
"""Get_Around_ML_predictions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ob5y3ucd22YLM8IJc9f8WaKLULlROE8q
"""



"""#**Get Around Predictive Machine Learning Models**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from sklearn.model_selection import  train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

from sklearn.inspection import permutation_importance
from sklearn.metrics import r2_score, mean_squared_error



import joblib
import os
import warnings
warnings.filterwarnings('ignore')

df= pd.read_csv('df.csv')
#pd.options.display.max_columns = None
#pd.options.display.max_rows = None

df.head()

df.info()

df.model_key.nunique()

"""Given up later (model_key there are 22 labels of cars. This may bias the prediction.
the model in somehow represented by horse power and by other features of the car. Which could be enough for the ML predictive model.)
"""

#df_clean= df.drop(['Unnamed: 0'], axis=1)

df_clean=df.copy()

"""#** Preprocesing**





"""

brands_to_keep = ['CitroÃ«n', 'Renault', 'BMW', 'Peugeot', 'Audi', 'Nissan', 'Mitsubishi', 'Mercedes', 'Volkswagen', 'Toyota', 'Ferrari', 'Porsche']

df_clean['model_key'] = df_clean['model_key'].where(df_clean['model_key'].isin(brands_to_keep), 'other_cars')

model_key_dummies  = pd.get_dummies(df_clean['model_key'], drop_first=True)

df_clean = df_clean.drop('model_key', axis=1)

df_clean = pd.concat([df_clean, model_key_dummies], axis=1)

X = df_clean.drop('rental_price_per_day', axis=1)
y = df_clean['rental_price_per_day']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.transform(X_test)
joblib.dump(scaler, 'C:/Users/serda/OneDrive/Bureau/Online Education/Certification/Get Around/api/scaler.joblib')

print(X_train.columns)
print(X_train.head(5))
print(scaled_X_train[:2])

"""#ML Models"""

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "XGBoost": xgb.XGBRegressor(random_state=42),
    "MLPRegressor": MLPRegressor(random_state=42, max_iter=500)
}

# Parameters for grid search
params = {
    'Random Forest': {
        'n_estimators': [100, 250],
        'max_depth': [4, 8]
    },
    'Gradient Boosting': {
        'n_estimators': [100, 250],
        'learning_rate': [0.05, 0.1],
        'max_depth': [4, 8],
        'subsample': [0.8],
    },
    'XGBoost': {
        'max_depth': [4, 8],
        'learning_rate': [0.05, 0.1],
        'n_estimators': [100, 250],
        'colsample_bytree': [0.9],
        'subsample': [0.8],
    }
}

scores = {
    "Model": [],
    "Root Mean Squared Error": [],
    "Mean Squared Error": [],
    "R^2 Score": [],
}

# Grid search, metrics and permutation importance
for model_name, model in models.items():
    if model_name in params:
        gridsearch = GridSearchCV(model, param_grid=params[model_name], cv=4)
        gridsearch.fit(scaled_X_train, y_train)
        print(f"Best parameters for {model_name}: {gridsearch.best_params_}")
        print(f"Best validation score for {model_name}: {gridsearch.best_score_}")
        model = gridsearch.best_estimator_

    model.fit(scaled_X_train, y_train)
    y_pred = model.predict(scaled_X_test)
    
    # save the XGBoost model , because it has the best results after the training returns
    if model_name == "XGBoost":
        joblib.dump(model, 'C:/Users/serda/OneDrive/Bureau/Online Education/Certification/Get Around/api/xgboost_model.joblib')

    
    y_pred = model.predict(scaled_X_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    scores["Model"].append(model_name)
    scores["Root Mean Squared Error"].append(rmse)
    scores["Mean Squared Error"].append(mse)
    scores["R^2 Score"].append(r2)

    models[model_name] = model

# Feature importance
fig = make_subplots(rows=len(models), cols=1, subplot_titles=[f"{model_name} Feature Importance" for model_name in models.keys()])

row = 1
for model_name, model in models.items():
    perm_importance = permutation_importance(model, scaled_X_test, y_test)
    sorted_idx = perm_importance.importances_mean.argsort()

    fig.add_trace(
        go.Bar(x=perm_importance.importances_mean[sorted_idx], y=X.columns[sorted_idx], orientation='h'),
        row=row, col=1
    )
    row += 1

fig.update_layout(height=300*len(models), width=1000, title_text="Feature Importance for each model")
fig.show()

scores_df=pd.DataFrame(scores)

print(scores_df)

